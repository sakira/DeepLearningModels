{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46b7880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "781713dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root='data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root='data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3578fb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9b24ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X[N, C, H, W]: torch.Size([64, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X[N, C, H, W]: {X.shape}\")\n",
    "    break\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "168f582d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# print(f\"Using {device} device\")\n",
    "\n",
    "# # Define model\n",
    "# class NeuralNetwork(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(NeuralNetwork, self).__init__()\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.linear_relu_stack = nn.Sequential(\n",
    "#             nn.Linear(28*28, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, 10)\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.flatten(x)\n",
    "#         logits = self.linear_relu_stack(x)\n",
    "#         return logits\n",
    "    \n",
    "\n",
    "# model = NeuralNetwork().to(device)\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c84f4366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16(\n",
      "  (conv11): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv21): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv22): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv31): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv32): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv33): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv41): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv42): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv43): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv51): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv52): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv53): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu): ReLU()\n",
      "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (fc3): Linear(in_features=4096, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#define model\n",
    "class VGG16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.conv11 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv12 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.conv21 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv22 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.conv31 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv32 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv33 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.conv41 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.conv42 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.conv43 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "                \n",
    "        self.conv51 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.conv52 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.conv53 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.fc1 = nn.Linear(25088, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, 1000)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # conv1\n",
    "        x = self.conv11(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv12(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        # conv2\n",
    "        x = self.conv21(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv22(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        # conv3\n",
    "        x = self.conv31(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv32(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv33(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        # conv4\n",
    "        x = self.conv41(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv42(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv43(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        # conv5\n",
    "        x = self.conv51(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv52(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv53(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "vgg16_model = VGG16().to(device)\n",
    "print(vgg16_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "110ee2e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1254e-02,  3.1288e-03,  1.0165e-02, -1.1046e-02,  1.6816e-03,\n",
       "         -7.9617e-03, -6.5243e-03,  2.1093e-03, -7.8734e-03, -1.6874e-02,\n",
       "          3.5261e-04,  3.5795e-03, -7.2717e-03,  1.2059e-02, -7.3453e-03,\n",
       "         -6.6724e-03,  6.5696e-03,  3.9058e-03,  1.2641e-02,  9.6298e-03,\n",
       "          2.3706e-03, -1.4010e-02,  2.1974e-03, -2.1487e-03, -4.1595e-03,\n",
       "         -2.3758e-02,  9.0624e-03,  1.4938e-02,  1.1911e-03, -8.0696e-03,\n",
       "         -4.6615e-03, -9.4701e-03,  1.4282e-02, -8.2764e-03, -3.3521e-03,\n",
       "         -2.4186e-02,  2.2476e-03, -1.9417e-02,  1.3808e-02,  1.2217e-02,\n",
       "          1.3703e-02, -7.9633e-05, -8.1627e-03, -1.0630e-02, -7.0152e-03,\n",
       "         -1.1487e-02,  8.7801e-03,  5.1405e-03,  1.7507e-03, -5.3565e-03,\n",
       "          1.6129e-02, -3.9638e-03,  4.6946e-03,  4.9449e-03, -2.5323e-03,\n",
       "          1.5351e-02,  4.4033e-03,  1.3211e-02,  4.9404e-03, -1.3794e-02,\n",
       "         -3.7725e-03,  7.9969e-04, -1.6749e-02, -3.4511e-03, -1.3036e-02,\n",
       "         -7.7411e-03,  8.2908e-04, -7.2283e-03, -1.4718e-02, -1.4038e-03,\n",
       "         -1.7612e-03, -1.8180e-03, -1.0903e-02,  7.8738e-03,  3.7040e-03,\n",
       "         -8.2418e-03, -2.4255e-02, -1.0277e-03, -1.2988e-02,  2.9231e-04,\n",
       "          3.1239e-03,  1.4785e-05, -6.6686e-03,  4.0619e-03, -1.6547e-02,\n",
       "         -1.5638e-02,  7.3273e-03, -2.1431e-02, -3.9615e-03,  9.4087e-03,\n",
       "         -5.6955e-03, -9.9386e-03, -1.3024e-03,  9.8082e-03,  8.3407e-03,\n",
       "          1.0131e-02, -7.0479e-03,  1.3060e-02, -7.7856e-03,  6.9437e-03,\n",
       "          2.4746e-03, -7.5096e-03, -1.0779e-02, -4.4666e-03,  9.3252e-03,\n",
       "          3.1927e-03, -1.2889e-02, -7.8338e-03, -2.7358e-03, -6.6882e-04,\n",
       "         -9.9603e-03,  6.2652e-03,  1.5132e-02,  2.1752e-03,  1.3078e-02,\n",
       "          3.6134e-04,  5.9881e-04,  1.1991e-02, -8.6597e-03,  3.5225e-03,\n",
       "          5.6069e-03, -5.0901e-03,  1.5471e-02,  1.6712e-02, -4.4558e-03,\n",
       "          4.7062e-04,  8.0739e-03,  7.7563e-03, -3.9458e-03,  1.3880e-02,\n",
       "          2.1118e-02, -1.2773e-02, -5.5771e-03, -9.2942e-03, -9.4487e-03,\n",
       "         -2.1553e-03,  8.1035e-03, -1.0785e-03,  8.0445e-03,  2.0607e-02,\n",
       "          1.2954e-02, -3.8942e-03, -5.5597e-03,  2.3885e-03, -1.5865e-02,\n",
       "          5.2915e-03, -1.6423e-02,  9.3607e-03, -8.3305e-03,  7.4655e-03,\n",
       "         -1.1437e-02, -2.0961e-02, -1.8555e-02, -5.7130e-03, -1.5804e-02,\n",
       "         -4.3980e-03, -1.0667e-02,  1.2961e-02, -9.8987e-03, -1.4080e-02,\n",
       "          9.1089e-03,  8.2989e-03, -1.2343e-02,  8.7497e-03, -1.5853e-02,\n",
       "         -8.3884e-03, -1.1458e-02, -1.9591e-03,  1.9460e-04,  7.5575e-03,\n",
       "          1.1846e-02, -1.9725e-02,  1.2803e-02,  3.8905e-03, -1.6547e-03,\n",
       "          6.9954e-03, -1.0342e-02,  4.5632e-03,  1.3498e-02, -1.6269e-02,\n",
       "         -2.3366e-03,  2.6957e-02,  1.5026e-02,  3.8747e-03,  1.1889e-02,\n",
       "          1.1093e-02, -1.1961e-02, -1.7809e-02, -7.4284e-03, -4.1210e-03,\n",
       "          4.7181e-03, -1.1474e-02,  8.3134e-04, -1.3623e-02,  2.4226e-04,\n",
       "          5.3191e-03,  4.9772e-03, -1.8621e-02,  1.4316e-02, -1.7219e-03,\n",
       "          1.8537e-02,  1.7263e-03, -9.4768e-03, -1.2476e-04, -1.9957e-02,\n",
       "         -5.2358e-03,  7.6944e-04, -3.4471e-03,  1.0509e-02, -1.2610e-02,\n",
       "         -7.8117e-03,  5.2955e-03, -1.3494e-02,  1.7097e-02,  1.9989e-02,\n",
       "         -5.2216e-04, -1.3557e-02,  1.3447e-02,  1.2477e-02, -6.2755e-03,\n",
       "          8.1864e-03,  1.7485e-02, -1.1133e-02, -5.7744e-03, -1.0852e-03,\n",
       "          1.3857e-02,  1.5720e-02, -1.4828e-02, -2.4376e-03, -1.7005e-03,\n",
       "          2.2044e-03,  3.9903e-03, -1.9217e-02,  4.6655e-03, -1.0985e-02,\n",
       "         -9.4807e-03,  8.3654e-03,  1.0884e-02, -1.7239e-02, -1.9421e-02,\n",
       "          6.1183e-03,  1.4099e-02,  1.5895e-02,  1.1622e-02, -5.8884e-03,\n",
       "         -6.2977e-03, -1.6716e-02, -6.5361e-03,  4.4429e-04, -1.0727e-02,\n",
       "          5.2344e-05,  1.1701e-03, -1.1126e-02, -5.9370e-03,  3.3790e-03,\n",
       "          1.3701e-02,  1.1843e-04, -5.0502e-04,  1.0454e-03,  8.7714e-04,\n",
       "         -7.6594e-03,  7.1974e-03, -5.8485e-03,  1.0050e-02, -6.3639e-04,\n",
       "          2.2242e-02,  1.6599e-02,  2.1459e-03, -1.9519e-02, -3.5170e-03,\n",
       "         -1.3548e-02,  1.6402e-03, -9.0591e-04, -1.8901e-02, -2.9425e-03,\n",
       "         -5.2089e-03, -1.0662e-02, -8.6340e-03, -6.3504e-03,  6.4102e-03,\n",
       "         -7.0517e-03,  8.8419e-03,  1.6993e-02, -5.3681e-03, -5.4211e-03,\n",
       "         -1.2387e-02,  1.3313e-02, -2.5018e-03,  1.4404e-03, -1.4118e-02,\n",
       "          3.3144e-03, -2.0596e-02, -5.4925e-04,  1.2948e-02, -8.1201e-03,\n",
       "         -1.1608e-02,  1.1325e-02,  1.0507e-02, -4.9761e-03, -5.7470e-03,\n",
       "         -1.7103e-02,  6.0628e-03, -1.2190e-02,  1.9330e-02, -1.4396e-02,\n",
       "          1.1196e-02,  1.0372e-02,  2.5220e-03,  9.1840e-04,  2.2793e-03,\n",
       "          2.8061e-04, -2.0948e-02,  1.3825e-03, -1.4835e-02,  5.2936e-03,\n",
       "          1.6126e-02, -3.1359e-03,  1.9074e-02, -9.1625e-03,  3.5989e-03,\n",
       "         -1.7468e-03, -6.4857e-03, -4.1547e-03,  2.6065e-03,  4.9027e-03,\n",
       "          1.3925e-02,  1.7073e-02,  1.8439e-02,  2.9528e-03, -3.1227e-03,\n",
       "         -1.9028e-02, -2.4314e-02, -7.0380e-03, -1.0884e-03,  1.3937e-02,\n",
       "         -9.1033e-03, -1.0337e-02,  1.0668e-02, -1.8385e-02,  1.5326e-03,\n",
       "         -1.5043e-03,  4.4030e-03, -1.3240e-02, -1.1547e-03,  6.2200e-03,\n",
       "         -4.2246e-03, -6.4651e-04, -6.0317e-03, -4.1305e-03, -2.8199e-03,\n",
       "          6.2054e-03, -2.5763e-04,  9.2569e-03,  1.0620e-02,  1.8537e-03,\n",
       "          8.8415e-04,  2.7174e-03, -1.1012e-02,  7.1897e-03, -2.8413e-03,\n",
       "         -1.4268e-02, -4.0691e-03, -2.3577e-02, -1.1007e-02,  9.4835e-03,\n",
       "          7.7235e-03, -1.1867e-02, -7.3728e-04,  1.4507e-02,  1.3956e-02,\n",
       "          1.1001e-02, -2.8403e-03,  6.7457e-04, -1.5668e-02, -1.1982e-02,\n",
       "          2.5902e-03, -2.8981e-03,  1.3493e-02,  1.3579e-02, -7.6052e-03,\n",
       "         -1.5213e-02, -1.2629e-02, -8.0427e-03, -1.3449e-02, -5.2162e-03,\n",
       "          1.0753e-03,  1.0477e-02, -4.5678e-03, -4.7279e-04, -2.4272e-03,\n",
       "         -2.4335e-04,  4.5358e-03,  3.9906e-03, -1.3254e-04,  2.3574e-03,\n",
       "          4.5108e-03, -4.9162e-03,  1.7928e-02, -1.4762e-02, -3.8530e-03,\n",
       "         -3.6985e-03,  5.2056e-03, -9.6372e-04, -8.9852e-03,  3.0774e-03,\n",
       "          2.7929e-03, -3.5702e-03, -2.1116e-02, -1.1899e-02, -9.5831e-03,\n",
       "         -2.8975e-04, -3.1819e-03, -9.0405e-03,  2.1629e-03, -3.6320e-03,\n",
       "          2.8576e-03, -1.0438e-02, -1.7433e-03,  1.0860e-02,  4.5208e-05,\n",
       "          1.6940e-02,  1.6499e-02, -1.1793e-02,  8.8283e-04,  4.1696e-03,\n",
       "         -9.8859e-03,  6.5713e-03,  1.6857e-02,  1.5334e-02,  1.7001e-02,\n",
       "          1.5467e-03, -8.8119e-03, -1.4896e-02,  4.7688e-03,  1.2780e-02,\n",
       "         -4.8127e-03,  1.7600e-02, -2.0638e-02, -7.3666e-03,  2.6732e-03,\n",
       "          1.9709e-02,  6.6988e-03, -1.6412e-02, -9.5916e-03,  1.6582e-02,\n",
       "         -1.2561e-02,  1.7960e-02, -1.4293e-02, -1.8056e-02,  1.1712e-02,\n",
       "          5.8838e-03, -7.3042e-03,  1.1874e-02, -1.3998e-02, -1.9178e-02,\n",
       "         -1.3838e-02, -9.6080e-03,  8.4738e-03,  5.0290e-03,  4.6376e-03,\n",
       "         -1.5669e-03, -2.7094e-04, -7.6378e-03, -9.4117e-03,  8.8248e-03,\n",
       "         -1.8455e-02, -5.7664e-03, -3.3015e-03, -3.0569e-02,  1.9751e-02,\n",
       "          7.8834e-03,  2.3059e-03,  4.0472e-04, -8.6054e-04, -5.3148e-03,\n",
       "         -1.6894e-02,  8.3882e-03, -3.0446e-03,  7.3319e-03,  1.1126e-02,\n",
       "          2.2544e-03,  6.4422e-03, -6.9281e-03, -6.4025e-03,  8.8407e-03,\n",
       "          1.4184e-02,  1.8262e-02, -1.5695e-02,  9.2407e-03,  3.6448e-03,\n",
       "         -4.7498e-03,  7.2632e-03, -6.4574e-03, -9.4113e-03, -1.4475e-03,\n",
       "         -1.0899e-02,  1.4270e-03, -1.2735e-02,  1.8704e-02,  1.1896e-02,\n",
       "          3.9708e-03, -7.3181e-03, -7.2799e-04, -6.3294e-03, -4.6057e-03,\n",
       "         -8.3210e-03, -1.7210e-02, -1.1311e-02,  1.3672e-03, -4.0666e-03,\n",
       "          3.9653e-03,  9.4609e-03, -5.5566e-03,  1.4697e-02,  9.5532e-03,\n",
       "         -2.6964e-03,  1.5974e-02,  1.1578e-02,  1.0023e-02, -2.2900e-03,\n",
       "          1.6927e-02,  9.2103e-04,  1.4232e-02, -5.5243e-03,  1.3766e-02,\n",
       "          3.1463e-03, -1.6151e-02, -2.5677e-04,  3.1948e-03,  7.8747e-05,\n",
       "         -5.9915e-03,  1.4994e-02,  1.0457e-02,  4.6366e-03, -1.5158e-02,\n",
       "          7.5511e-03, -2.1317e-02,  1.6828e-02, -1.5280e-02,  1.9025e-03,\n",
       "          4.8721e-03, -1.0518e-02,  6.1996e-03,  6.2934e-03, -3.4745e-04,\n",
       "         -9.2635e-03, -2.2155e-03,  1.4703e-02, -6.4497e-03, -3.5975e-03,\n",
       "          7.5773e-04,  1.8593e-02, -4.7515e-03,  4.3058e-03, -1.2113e-02,\n",
       "         -1.7644e-03, -2.1168e-03,  1.3133e-03, -8.3658e-03, -4.2992e-03,\n",
       "         -7.8092e-03, -1.7333e-02,  7.7835e-03, -2.2966e-02, -1.1246e-02,\n",
       "         -1.4392e-02, -1.9303e-03,  1.0991e-02,  1.6813e-02,  3.4484e-03,\n",
       "         -6.8501e-03,  1.1494e-02, -2.4823e-03, -6.9550e-03,  1.4647e-02,\n",
       "          2.3622e-02, -7.7269e-03, -7.9458e-03, -1.9143e-02,  3.2094e-03,\n",
       "          3.0013e-03, -6.8082e-03,  9.3220e-03, -2.2991e-03,  7.6583e-03,\n",
       "          7.2615e-03,  9.2332e-04,  2.1073e-02, -2.1935e-03, -8.3231e-03,\n",
       "         -7.6280e-03,  8.8721e-03,  9.8084e-03,  9.3314e-03,  1.7044e-03,\n",
       "         -1.1873e-02, -1.9051e-02, -1.2027e-02, -9.9464e-03, -1.1334e-02,\n",
       "         -4.6152e-03, -8.5222e-03, -1.8171e-02, -1.6793e-02,  1.3695e-02,\n",
       "          1.1626e-03, -1.0364e-02,  9.4852e-03, -1.4016e-02,  3.5850e-04,\n",
       "         -2.9158e-03, -4.1720e-03,  1.4065e-03, -2.2723e-03,  1.9756e-03,\n",
       "          1.0510e-02,  9.4698e-03, -2.6546e-03, -1.6314e-02,  1.5524e-02,\n",
       "          6.8535e-03,  6.2488e-03,  6.7575e-03, -1.8114e-02,  5.0500e-04,\n",
       "         -3.5338e-03, -3.8436e-03, -2.5998e-03, -2.2849e-03, -3.1719e-03,\n",
       "         -7.7881e-03,  7.5815e-03, -1.4630e-02,  1.3961e-03,  1.0726e-02,\n",
       "          1.0872e-02,  2.5529e-03,  1.6380e-02,  1.6546e-02, -6.7804e-03,\n",
       "          1.6033e-02, -1.7868e-03,  2.4025e-03,  1.9370e-02,  7.6634e-03,\n",
       "         -4.2458e-03,  6.2897e-03,  1.6093e-02, -2.8251e-03, -2.7863e-03,\n",
       "          1.2659e-02,  1.1578e-02, -1.4716e-02, -3.8414e-03, -4.7569e-04,\n",
       "         -8.1764e-03,  1.3257e-03, -1.1333e-02, -1.2294e-02, -1.4390e-02,\n",
       "         -1.6931e-02, -5.5356e-03,  1.8268e-02,  3.5688e-03, -1.4656e-02,\n",
       "          1.6354e-02, -6.6188e-03,  1.2882e-02,  1.1840e-02,  6.2727e-03,\n",
       "          1.4041e-03, -1.0940e-02,  1.0407e-02,  6.9094e-03,  9.0921e-04,\n",
       "          1.2881e-02,  1.6296e-02, -8.9217e-03,  1.3173e-02,  7.2738e-03,\n",
       "         -9.1561e-03, -1.4928e-02, -4.0396e-03,  6.5209e-03, -1.1974e-02,\n",
       "          5.0186e-03,  3.0271e-03, -6.1899e-03,  9.5031e-03,  6.7436e-03,\n",
       "         -2.3566e-03,  1.1379e-02,  8.6440e-03,  9.9590e-03, -2.4520e-02,\n",
       "          9.8676e-03,  1.6188e-02, -9.6787e-03, -1.4346e-02,  2.9018e-03,\n",
       "         -4.1189e-03, -4.9817e-03, -1.8730e-02,  7.0994e-03,  9.9055e-03,\n",
       "          8.3658e-03,  7.8446e-03,  8.2526e-03, -5.0894e-03,  8.0517e-03,\n",
       "          6.9115e-03,  5.8908e-03,  1.6729e-02,  8.7499e-03, -1.0372e-02,\n",
       "         -8.9083e-03, -7.8993e-03,  1.3257e-03, -5.4106e-03,  2.0878e-03,\n",
       "          3.1716e-03,  2.2573e-02,  1.1778e-02, -2.5242e-03, -4.4461e-03,\n",
       "         -3.9676e-03, -4.0753e-03,  3.1414e-04,  8.6036e-03, -1.1922e-02,\n",
       "         -2.7057e-03, -4.2556e-03,  2.6490e-02,  3.5094e-04,  5.6162e-03,\n",
       "         -5.6008e-03, -9.0938e-03,  3.1836e-03,  1.0405e-02, -1.4713e-02,\n",
       "          8.4528e-03,  2.3227e-03, -1.5743e-02, -3.5813e-03, -1.0442e-02,\n",
       "         -2.2784e-02, -3.0227e-03,  1.7328e-02, -8.0371e-03,  8.6034e-03,\n",
       "         -1.2102e-02,  3.6230e-03,  3.6506e-03, -9.4051e-03, -2.1438e-02,\n",
       "         -1.0536e-02,  3.3181e-03, -1.3499e-02, -1.2613e-02,  3.8194e-04,\n",
       "          3.1667e-03, -1.7792e-02, -8.2052e-03, -7.0220e-03, -1.2066e-02,\n",
       "         -1.1588e-02, -3.2811e-03,  1.9170e-02,  4.0240e-03,  1.6928e-02,\n",
       "          4.2738e-03, -1.5095e-02,  9.4995e-03,  1.2167e-04, -1.7850e-02,\n",
       "         -2.5640e-03, -1.4311e-03,  1.1831e-02, -9.5475e-03,  5.1522e-03,\n",
       "         -1.4295e-02, -1.5043e-03, -8.0341e-03,  6.4821e-03,  8.2836e-03,\n",
       "         -6.4370e-03, -8.9984e-03, -1.7425e-03,  2.0933e-03,  1.7626e-02,\n",
       "         -9.4844e-03,  1.2899e-02, -4.8389e-03,  1.9664e-02, -1.3852e-02,\n",
       "         -3.3634e-03, -9.0269e-03,  1.3667e-02,  6.5745e-03,  1.3946e-02,\n",
       "         -2.1204e-03,  1.7573e-02, -6.6147e-03,  1.6724e-02,  1.4306e-02,\n",
       "         -8.1779e-03,  1.0524e-02, -2.3295e-03, -4.8826e-03, -1.9567e-03,\n",
       "         -1.3071e-02, -9.1550e-03,  6.1081e-03, -1.0145e-03, -1.3219e-02,\n",
       "         -1.0928e-02,  2.1396e-02,  5.7801e-03, -7.3208e-03, -2.3731e-02,\n",
       "          9.6156e-03, -1.8487e-02, -8.7082e-03, -1.2341e-02,  5.0850e-03,\n",
       "         -7.3205e-03, -6.8998e-03, -5.7455e-03,  1.7239e-04, -5.2163e-04,\n",
       "          2.2362e-04, -1.6092e-02,  1.6094e-02,  2.9728e-03,  1.4629e-02,\n",
       "          1.8008e-02, -1.2551e-02,  1.0791e-02, -1.1949e-02, -1.6931e-02,\n",
       "         -8.9202e-03, -1.2240e-02, -1.0849e-02, -2.6402e-03, -4.2790e-03,\n",
       "         -8.9166e-03,  5.0577e-03,  1.9240e-03, -2.1254e-03,  2.0283e-03,\n",
       "         -3.9791e-03, -6.6737e-03, -6.3466e-03, -4.4185e-03,  5.5391e-03,\n",
       "         -8.1763e-03, -1.1275e-03, -3.7247e-03,  1.1575e-02,  7.7944e-04,\n",
       "         -2.1082e-02,  5.9150e-03, -6.1934e-03,  6.1052e-05,  4.1017e-03,\n",
       "          1.0295e-02, -2.0170e-03,  8.6069e-03,  3.6421e-03, -6.7328e-03,\n",
       "          5.6078e-03, -1.1128e-02,  8.8843e-03, -7.4739e-03,  1.3081e-02,\n",
       "          1.9328e-03, -1.1220e-02, -9.4008e-03,  3.9403e-03, -1.6250e-02,\n",
       "         -1.6892e-02,  8.8310e-03, -2.5520e-03, -6.8017e-03,  2.3835e-03,\n",
       "         -1.7304e-03, -8.1519e-03, -4.5209e-03, -1.1814e-02, -1.9269e-02,\n",
       "          9.5817e-03, -7.3545e-03, -1.2781e-02,  8.6266e-03, -1.6848e-03,\n",
       "         -1.7470e-02,  1.1183e-02,  3.3386e-03, -1.4655e-02, -1.3117e-02,\n",
       "         -5.2949e-03,  7.0392e-03,  9.2243e-03,  1.0371e-02,  5.8048e-03,\n",
       "          6.8216e-03, -3.8148e-03,  9.8675e-03, -3.3241e-03,  5.5312e-03,\n",
       "         -9.7948e-04, -1.7340e-02, -2.0741e-03,  1.2617e-02, -1.1345e-03,\n",
       "         -1.0742e-02,  1.1311e-02, -1.2288e-02,  1.3822e-02, -2.7380e-04,\n",
       "         -4.5006e-03, -2.2018e-02,  7.7456e-03, -1.1058e-02,  4.6371e-03,\n",
       "         -1.0352e-02,  1.9421e-03,  7.9240e-03,  4.8914e-03,  2.0955e-02,\n",
       "          1.0589e-02, -9.2273e-03,  5.2710e-03, -1.3704e-02,  2.1761e-02,\n",
       "          1.0587e-02,  1.1561e-02,  8.5084e-03, -1.0777e-02, -9.3103e-03,\n",
       "          6.2232e-03, -9.3460e-03,  3.8161e-03, -2.4261e-03,  4.2988e-03,\n",
       "          2.0675e-02, -3.7866e-04,  5.4215e-03,  8.5763e-03,  6.1869e-03,\n",
       "          7.1788e-03, -5.4881e-03, -9.3159e-03,  6.0706e-03, -5.5241e-03,\n",
       "         -1.9968e-02,  1.1236e-02,  4.3643e-03,  1.5716e-02,  1.0630e-02,\n",
       "          1.6822e-02,  1.7063e-02, -7.8343e-03,  3.7658e-03,  6.2974e-04,\n",
       "         -4.3826e-03, -1.9830e-02,  1.2420e-02,  2.2715e-03,  1.7819e-03,\n",
       "          1.8281e-04,  7.8809e-03, -4.3818e-03,  3.9815e-03, -1.5677e-02,\n",
       "         -1.1058e-02,  1.9548e-02,  1.1546e-02,  5.0694e-03, -1.5229e-03,\n",
       "          1.8112e-02, -2.6681e-02,  1.6630e-02,  7.6485e-03,  3.1595e-03,\n",
       "          8.7215e-03,  7.1643e-03, -7.3205e-03, -1.5238e-02, -1.3914e-02,\n",
       "          6.6476e-03, -1.5203e-02,  8.1277e-03, -1.2115e-02, -1.6039e-03,\n",
       "         -8.4936e-03,  1.7480e-03, -4.6980e-03, -8.0155e-03,  1.1981e-02]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "X = torch.rand(1, 3, 224, 224).to(device)\n",
    "pred = vgg16_model(X)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83448324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.vgg16()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4365b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2 = model(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec3711de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(pred2, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c260157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1866, grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(pred2-pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
